{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371768a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aniket\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.3)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd19b54",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d467a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cat': 0, 'Dog': 1}\n",
      "['Cat', 'Dog']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "data_path='./Dataset/'\n",
    "categories=os.listdir(data_path)\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7fc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "\n",
    "        try:  \n",
    "            resized=cv2.resize(img,(img_size,img_size))\n",
    "            #resizing the image  into 100x100, since we need a fixed common size for all the images in the dataset\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "            #appending the image and the label(categorized) into the list (dataset)\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)\n",
    "            #if any exception rasied, the exception will be printed here. And pass to the next image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940669c",
   "metadata": {},
   "source": [
    "# Recale and assign catagorical lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac95616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,3))\n",
    "target=np.array(target)\n",
    "from keras.utils import np_utils\n",
    "new_target=np_utils.to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c2e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a272596",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee10a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 100, 100, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9582f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23aa8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(200,(3,3),input_shape=data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "#Flatten layer to stack the output convolutions from second convolution layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "#Dense layer of 64 neurons\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#The Final layer with two outputs for two categories\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b4d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 200)       5600      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 98, 98, 200)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 200)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 100)       180100    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 47, 47, 100)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 52900)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 52900)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2645050   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,830,852\n",
      "Trainable params: 2,830,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d76e25",
   "metadata": {},
   "source": [
    "# Splitting data into traning and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eeeba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,new_target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16986d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 100, 100, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd11abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30cc034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23/23 [==============================] - 104s 4s/step - loss: 0.7631 - accuracy: 0.5083 - val_loss: 0.6932 - val_accuracy: 0.4778\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.6900 - accuracy: 0.5444 - val_loss: 0.7380 - val_accuracy: 0.4167\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.6979 - accuracy: 0.5500 - val_loss: 0.6927 - val_accuracy: 0.5111\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 113s 5s/step - loss: 0.6871 - accuracy: 0.5792 - val_loss: 0.6853 - val_accuracy: 0.5500\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.6582 - accuracy: 0.6222 - val_loss: 0.7823 - val_accuracy: 0.4056\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 115s 5s/step - loss: 0.6378 - accuracy: 0.6111 - val_loss: 0.6889 - val_accuracy: 0.5056\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.5995 - accuracy: 0.6694 - val_loss: 0.6939 - val_accuracy: 0.5833\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 108s 5s/step - loss: 0.5551 - accuracy: 0.7139 - val_loss: 0.7730 - val_accuracy: 0.5389\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.4982 - accuracy: 0.7444 - val_loss: 0.8410 - val_accuracy: 0.5778\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 97s 4s/step - loss: 0.4211 - accuracy: 0.7847 - val_loss: 1.0152 - val_accuracy: 0.5722\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.4147 - accuracy: 0.7931 - val_loss: 0.7907 - val_accuracy: 0.6111\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.3526 - accuracy: 0.8431 - val_loss: 1.0220 - val_accuracy: 0.6056\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.3433 - accuracy: 0.8542 - val_loss: 0.8166 - val_accuracy: 0.6389\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.3023 - accuracy: 0.8611 - val_loss: 0.9910 - val_accuracy: 0.6222\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.3045 - accuracy: 0.8681 - val_loss: 0.9671 - val_accuracy: 0.6056\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 100s 4s/step - loss: 0.2359 - accuracy: 0.8931 - val_loss: 1.0486 - val_accuracy: 0.6167\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 97s 4s/step - loss: 0.2482 - accuracy: 0.9000 - val_loss: 0.9629 - val_accuracy: 0.5833\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 80s 4s/step - loss: 0.2447 - accuracy: 0.9056 - val_loss: 1.2739 - val_accuracy: 0.5389\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 83s 4s/step - loss: 0.2100 - accuracy: 0.9167 - val_loss: 1.4489 - val_accuracy: 0.5778\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 80s 4s/step - loss: 0.1957 - accuracy: 0.9153 - val_loss: 1.2473 - val_accuracy: 0.5611\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_data,train_target,epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f6566",
   "metadata": {},
   "source": [
    "# Saving The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c4abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Trained_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001fe0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
